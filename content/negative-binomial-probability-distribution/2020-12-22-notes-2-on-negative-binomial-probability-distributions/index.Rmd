---
title: Notes 2 on Negative Binomial Probability Distributions
author: ~
date: '2020-12-22'
slug: notes-2-on-negative-binomial-probability-distributions
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2020-12-22T22:02:18-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
math: true
draft: false
three: true
---

<div id='ctns-3d-graph'></div>
<div id='MY_GRAPH'></div>
<div id='WebGL-output-X'></div>
<div id='Stats-output-X'></div>

```{js}
document.addEventListener("THREE", function(event) {

    var TML_MATH3D = window.TML_MATH3D || {};
    var handleQ = $('#MY_GRAPH');

    // My-Graph-3
    //
    TML_MATH3D.init({})
        .createCylinderSet({
            style:"rectangle",
            plane:"xy-plane",
            rotation:"primary",
            color:"red",
            addToScene:true,
            addToRotation:true,
            pickOne:false})
        .render();

  });
```

### Wiki has it wrong - please confirm

A <strong>negative binomial distribution</strong> is a discrete probability distribution that models the number of [failures] in a sequence  of independent and identically distributed Bernoulli trials before a specified [fixed] number of [successes] ... occurs (“Negative Binomial Distribution”, corrected).

Suppose there is a sequence of independent Bernoulli trials. Thus, each trial has two potential outcomes called "success" and "failure". In each trial the probability of success is $p$ and of failure is $(1 − p)$. We are observing this sequence until a predefined number $r$ of [successes] have occurred. Then the random number of [failures] we have seen, $X$, will have the negative binomial (or Pascal) distribution (“Negative Binomial Distribution”, corrected).

The probability mass function (<strong>pmf</strong>) of the negative binomial distribution is

\begin{equation}
P(X = k \ | \ r,p) = { k+r-1 \choose r-1} p^r (1-p)^k 
\end{equation}

where $k$ is the number of failures, $r$ is the specified number of successes, and $p$ is the specified probability of success.

----

```{definition, name="Negative Binomial Probability Distribution"}
A random variable $X$ is said to have a <em>negative binomial probability distribution</em> if and only if

\begin{equation}
P(X=k \ | \ r,p) = {k+r-1 \choose r-1 } p^r (1-p)^k
\end{equation}

where $k$ is the number of failures, $r$ is the specified number of successes, and $p$ is the specified probability of success.
```

----

```{definition, name="Negative Binomial Probability Distribution"}
A random variable $Y$ is said to have a <em>negative binomial probability distribution</em> if and only if

\begin{align}
\ \ \ \ P(Y=y \ | \ r,p) = {y-1 \choose r-1 } p^r (1-p)^{y-r}
\end{align}

for

\begin{align}
y = r, r+1, r+2, ..., \text{ and } 0 \le p \le 1
\end{align}

where $y$ is the number of trials, $r$ is the given number of successes, and $p$ is the given probability of success.
```

----

```{python, echo=FALSE}
import matplotlib.pyplot as plt

plt.plot([1, 2, 3, 4])
plt.ylabel('some numbers')
plt.show()

```

----

# Personal Notes

The negative probability distribution is equivalent to the geometric distribution when the number of successes equals one. That is, a random variable with a negative binomial probability distribution originates from a context similar to that of a random variable with a geometric distribution:

- The random variable is based on a series of independent Bernoulli TRIALS. 
- Each Bernoulli trial has one of two possible outcomes: SUCCESS or FAILURE. 

The geometric and negative binomial distributions differ as to their desired final outcome:

- Geometric distribution: The random variable equals the total number of trials up to and including the first SUCCESS;
- Negative Binomial (<strong>NOT EXACTLY</strong>): The random variable equals the total number of trials up to and including the $n^{th}$ SUCCESS.

<strong>Not Exactly:</strong> Clearly, a negative binomial distribution equals the geometric distribution when the $Nth$ trial IS the $1st$ TRIAL.

----

Another way to look at the <strong>pmf</strong> is to arrange the sequence of trials into two Events $A$ and $B$, where
the Event $A$ contains all the trials up to but not including the last trial and Event $B$ contains only the last trial. See <strong>Figure 1</strong> below.

![<strong>Figure 1.</strong> Separate the trials into two Events A and B.](/negative-binomial-distribution/example-2.png)

Event $A$ contains the number of trials equal to the total number of failures $k$ plus the total number of successes $r$ minus one,

\begin{align}
N[A] = & k+r-1 \\\\[0.5em]
     = & k + (r-1) \\\\
\end{align}

The probability of Event $A$ is the total probability of any combination of $k$ failures and $r-1$ successes,

\begin{equation}
Pr[A] = {k+r-1 \choose r-1 } (1-p)^k p^{r-1}
\end{equation}

The combination denotes the total number of ways to choose $k+r-1$ trials taken $r-1$ at a time. Essentially, I have $k+r-1$ trials, and then I am allocating $r-1$ successes among those trials.

And for any one of those combinations, I have the combined probability of the $k$ independent failures, $(1-p)^k$, times the combined probability of the $r-1$ independent successes.

The probability of Event $B$ is simply

\begin{equation}
Pr[B] = p
\end{equation}

And because we assume these trials are independent, the total probability is simply

\begin{align}
Pr[X=k] = & Pr[A] \cdot Pr[B] \\\\[1em]
        = & \left[ {k+r-1 \choose r-1 } (1-p)^k p^{r-1} \right] \cdot Pr[B] \\\\[1em]
        = & \left[ {k+r-1 \choose r-1 } (1-p)^k p^{r-1} \right] \cdot  p \\\\[1em]
        = & {k+r-1 \choose r-1 } (1-p)^k p^{r-1} \cdot p \\\\[1em]
        = & {k+r-1 \choose r-1 } (1-p)^k p^r
\end{align}

----

From (Wackerly et al., p.116), let us select fixed values for $y$ and $r$ and consider events $A$ and $B$, where

$
\ \ \ \ \ A = \\{ \text{ the first } (y-1) \text{ trials contain } (r-1) \text{ successes } \\}
$

and

$
\ \ \ \ \ B = \\{ \text{ trial } y \text{ results in a success } \\}
$

Precisely because the events $A$ and $B$ are independent, we can say

$$P[ A \cap B] = P[A] \times P[B]$$

Given a random variable $Y$ with a negative binomial probability distribution and the desired number of $r$ TRIALS that yield SUCCESS, the probability $P$ that $Y$ will equal $y$ trials is given by the expression:

$$P[Y=y] = {y-1 \choose r-1}p^rq^{y-r}$$

where $p$ is the probability of SUCCESS in a trial, $q=1-p$ is the probability of FAILURE, $r$ is the total number of trials, and $y$ is the total number of trials that yield a SUCCESS.

----

### In-Text Citations

(Wackerly et al.)

(“Negative Binomial Distribution”)

### References

Wackerly, Dennis D, et al. Mathematical Statistics with Applications. 2002. 6th ed., Belmont, Calif., Brooks-Cole, 2008.

“Negative Binomial Distribution.” Wikipedia, 18 Dec. 2020, en.wikipedia.org/wiki/Negative_binomial_distribution. Accessed 22 Dec. 2020.

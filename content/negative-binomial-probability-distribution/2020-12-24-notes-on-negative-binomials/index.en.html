---
title: Notes on Negative Binomials
author: ''
date: '2020-12-24'
slug: notes-on-negative-binomials
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2020-12-24T13:45:35-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
math: true
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<p>A <strong>negative binomial distribution</strong> is a discrete probability distribution that models the number of [failures] in a sequence of independent and identically distributed Bernoulli trials before a specified [fixed] number of [successes] … occurs (“Negative Binomial Distribution”, <strong>corrected</strong>).</p>
<p>Suppose there is a sequence of independent Bernoulli trials, [which means] each trial has two potential outcomes called “success” and “failure”. In each trial the probability of success is <span class="math inline">\(p\)</span>, [while the probability] … of failure is <span class="math inline">\((1 − p)\)</span>. We are observing this sequence until a predefined number <span class="math inline">\(r\)</span> of [successes] have occurred. Then the random number of [failures] we have seen, <span class="math inline">\(X\)</span>, will have the negative binomial (or Pascal) distribution (“Negative Binomial Distribution”, <strong>corrected</strong>).</p>
<hr />
<p>The probability mass function (<strong>pmf</strong>) of the negative binomial distribution is given by <strong>Equation <a href="#eq:negbinom1">(1)</a></strong></p>
<p><span class="math display" id="eq:negbinom1">\[\begin{equation}
P(X = x \ | \ r,p) = { x+r-1 \choose r-1} p^r (1-p)^x 
\tag{1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(x\)</span> is the number of failures, <span class="math inline">\(r\)</span> is the number of successes, and <span class="math inline">\(p\)</span> is the probability of success.</p>
<hr />
<p>Alternatively, the random number of trials we have seen, <span class="math inline">\(Y\)</span>, will also have a negative binomial distribution. In this case, if we let <span class="math inline">\(y=k+r\)</span>, then the probability mass function of <span class="math inline">\(Y\)</span> is given by <strong>Equation <a href="#eq:negbinom2">(2)</a></strong>.</p>
<p><span class="math display" id="eq:negbinom2">\[\begin{equation}
P(Y = y \ | \ r,p) = { y-1 \choose r-1} p^r (1-p)^{y-r} 
\tag{2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the number of trials, <span class="math inline">\(y-r\)</span> is the number of failures, <span class="math inline">\(r\)</span> is the number of successes, and <span class="math inline">\(p\)</span> is the specified probability of success.</p>
<hr />

<div class="definition">
<p><span id="def:unnamed-chunk-1" class="definition"><strong>Definition 1  (Negative Binomial Probability Distribution)  </strong></span>A random variable <span class="math inline">\(X\)</span> is said to have a <em>negative binomial probability distribution</em> if and only if</p>
<p><span class="math display">\[\begin{equation}
P(X=x \ | \ r,p) = {x+r-1 \choose r-1 } p^r (1-p)^x
\end{equation}\]</span></p>
where <span class="math inline">\(x\)</span> is the number of failures, <span class="math inline">\(r\)</span> is the given number of successes, and <span class="math inline">\(p\)</span> is the given probability of success.
</div>

<hr />

<div class="definition">
<p><span id="def:unnamed-chunk-2" class="definition"><strong>Definition 2  (Negative Binomial Probability Distribution)  </strong></span>A random variable <span class="math inline">\(Y\)</span> is said to have a <em>negative binomial probability distribution</em> if and only if</p>
<p><span class="math display">\[\begin{align}
\ \ \ \ P(Y=y \ | \ r,p) = {y-1 \choose r-1 } p^r (1-p)^{y-r}
\end{align}\]</span></p>
<p>for</p>
<p><span class="math display">\[\begin{align}
y = r, r+1, r+2, ..., \text{ and } 0 \le p \le 1
\end{align}\]</span></p>
where <span class="math inline">\(y\)</span> is the number of trials, <span class="math inline">\(r\)</span> is the given number of successes, and <span class="math inline">\(p\)</span> is the given probability of success.
</div>

<hr />

<div class="theorem">
<p><span id="thm:unnamed-chunk-3" class="theorem"><strong>Theorem 1  (Negative Binomial Probability Distribution)  </strong></span>If <span class="math inline">\(X\)</span> is a random variable with a negative binomial distribution,</p>
<p><span class="math display">\[\begin{equation}
\mu = E(X) = \frac {rq}{p} \ \ \text{ and } \ \ \sigma^2 = V(X) = \frac {rq}{p^2}.
\end{equation}\]</span></p>
where <span class="math inline">\(x\)</span> is the number of failures, <span class="math inline">\(r\)</span> is the given number of successes,<span class="math inline">\(p\)</span> is the given probability of success, and <span class="math inline">\(q=(1-p)\)</span> is the probability of failure.
</div>

<hr />

<div class="theorem">
<p><span id="thm:unnamed-chunk-4" class="theorem"><strong>Theorem 2  (Negative Binomial Probability Distribution)  </strong></span>If <span class="math inline">\(Y\)</span> is a random variable with a negative binomial distribution,</p>
<p><span class="math display">\[\begin{equation}
\mu = E(Y) = \frac rp \ \ \text{ and } \ \ \sigma^2 = V(Y) = \frac {rq}{p^2}.
\end{equation}\]</span></p>
where <span class="math inline">\(y\)</span> is the number of trials, <span class="math inline">\(r\)</span> is the given number of successes, <span class="math inline">\(p\)</span> is the given probability of success, and <span class="math inline">\(q=(1-p)\)</span> is the probability of failure.
</div>

<hr />
<p><img src="/negative-binomial-probability-distribution/2020-12-24-notes-on-negative-binomials/index.en_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<hr />
<p>Computing the average value</p>
<pre><code>## mu=1.25, var=1.56 by equation OUCH!</code></pre>
<pre><code>## mu=20.00, var=100.00 by custom</code></pre>
<pre><code>## mu=20.00, var=100.00 by nbinom</code></pre>
<hr />
<div id="personal-notes" class="section level1">
<h1>Personal Notes</h1>
<p>The negative probability distribution is equivalent to the geometric distribution when the number of successes equals one. That is, a random variable with a negative binomial probability distribution originates from a context similar to that of a random variable with a geometric distribution:</p>
<ul>
<li>The random variable is based on a series of independent Bernoulli TRIALS.</li>
<li>Each Bernoulli trial has one of two possible outcomes: SUCCESS or FAILURE.</li>
</ul>
<p>The geometric and negative binomial distributions differ as to their desired final outcome:</p>
<ul>
<li>Geometric distribution: The random variable equals the total number of trials up to and including the first SUCCESS;</li>
<li>Negative Binomial (<strong>NOT EXACTLY</strong>): The random variable equals the total number of trials up to and including the <span class="math inline">\(n^{th}\)</span> SUCCESS.</li>
</ul>
<p><strong>Not Exactly:</strong> Clearly, a negative binomial distribution equals the geometric distribution when the <span class="math inline">\(Nth\)</span> trial IS the <span class="math inline">\(1st\)</span> TRIAL.</p>
<hr />
<p>Another way to look at the <strong>pmf</strong> is to arrange the sequence of trials into two Events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, where
the Event <span class="math inline">\(A\)</span> contains all the trials up to but not including the last trial and Event <span class="math inline">\(B\)</span> contains only the last trial. See <strong>Figure 1</strong> below.</p>
<div class="figure">
<img src="/negative-binomial-distribution/example-2.png" alt="" />
<p class="caption"><strong>Figure 1.</strong> Separate the trials into two Events A and B.</p>
</div>
<p>Event <span class="math inline">\(A\)</span> contains the number of trials equal to the total number of failures <span class="math inline">\(k\)</span> plus the total number of successes <span class="math inline">\(r\)</span> minus one,</p>
<p><span class="math display">\[\begin{align}
N[A] = &amp; k+r-1 \\\\[0.5em]
     = &amp; k + (r-1) \\\\
\end{align}\]</span></p>
<p>The probability of Event <span class="math inline">\(A\)</span> is the total probability of any combination of <span class="math inline">\(k\)</span> failures and <span class="math inline">\(r-1\)</span> successes,</p>
<p><span class="math display">\[\begin{equation}
Pr[A] = {k+r-1 \choose r-1 } (1-p)^k p^{r-1}
\end{equation}\]</span></p>
<p>The combination denotes the total number of ways to choose <span class="math inline">\(k+r-1\)</span> trials taken <span class="math inline">\(r-1\)</span> at a time. Essentially, I have <span class="math inline">\(k+r-1\)</span> trials, and then I am allocating <span class="math inline">\(r-1\)</span> successes among those trials.</p>
<p>And for any one of those combinations, I have the combined probability of the <span class="math inline">\(k\)</span> independent failures, <span class="math inline">\((1-p)^k\)</span>, times the combined probability of the <span class="math inline">\(r-1\)</span> independent successes.</p>
<p>The probability of Event <span class="math inline">\(B\)</span> is simply</p>
<p><span class="math display">\[\begin{equation}
Pr[B] = p
\end{equation}\]</span></p>
<p>And because we assume these trials are independent, the total probability is simply</p>
<p><span class="math display">\[\begin{align}
Pr[X=k] = &amp; Pr[A] \cdot Pr[B] \\\\[1em]
        = &amp; \left[ {k+r-1 \choose r-1 } (1-p)^k p^{r-1} \right] \cdot Pr[B] \\\\[1em]
        = &amp; \left[ {k+r-1 \choose r-1 } (1-p)^k p^{r-1} \right] \cdot  p \\\\[1em]
        = &amp; {k+r-1 \choose r-1 } (1-p)^k p^{r-1} \cdot p \\\\[1em]
        = &amp; {k+r-1 \choose r-1 } (1-p)^k p^r
\end{align}\]</span></p>
<hr />
<p>From (Wackerly et al., p.116), let us select fixed values for <span class="math inline">\(y\)</span> and <span class="math inline">\(r\)</span> and consider events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, where</p>
<p>$
     A = \{  (y-1)  (r-1)  \}
$</p>
<p>and</p>
<p>$
     B = \{  y  \}
$</p>
<p>Precisely because the events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, we can say</p>
<p><span class="math display">\[P[ A \cap B] = P[A] \times P[B]\]</span></p>
<p>Given a random variable <span class="math inline">\(Y\)</span> with a negative binomial probability distribution and the desired number of <span class="math inline">\(r\)</span> TRIALS that yield SUCCESS, the probability <span class="math inline">\(P\)</span> that <span class="math inline">\(Y\)</span> will equal <span class="math inline">\(y\)</span> trials is given by the expression:</p>
<p><span class="math display">\[P[Y=y] = {y-1 \choose r-1}p^rq^{y-r}\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the probability of SUCCESS in a trial, <span class="math inline">\(q=1-p\)</span> is the probability of FAILURE, <span class="math inline">\(r\)</span> is the total number of trials, and <span class="math inline">\(y\)</span> is the total number of trials that yield a SUCCESS.</p>
<hr />
<div id="in-text-citations" class="section level3">
<h3>In-Text Citations</h3>
<p>(Wackerly et al.)</p>
<p>(“Negative Binomial Distribution”)</p>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<p>Wackerly, Dennis D, et al. Mathematical Statistics with Applications. 2002. 6th ed., Belmont, Calif., Brooks-Cole, 2008.</p>
<p>“Negative Binomial Distribution.” Wikipedia, 18 Dec. 2020, en.wikipedia.org/wiki/Negative_binomial_distribution. Accessed 22 Dec. 2020.</p>
</div>
</div>

---
title: Notes on Negative Binomials
author: ''
date: '2020-12-24'
slug: notes-on-negative-binomials
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2020-12-24T13:45:35-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
math: true
---
A <strong>negative binomial distribution</strong> is a discrete probability distribution that models the number of [failures] in a sequence  of independent and identically distributed Bernoulli trials before a specified [fixed] number of [successes] ... occurs (“Negative Binomial Distribution”, <strong>corrected</strong>).

Suppose there is a sequence of independent Bernoulli trials, [which means] each trial has two potential outcomes called "success" and "failure". In each trial the probability of success is $p$, [and therefore the probability] ... of failure is $(1 − p)$. We are observing this sequence until a predefined number $r$ of [successes] have occurred. Then the random number of [failures] we have seen, $X$, will have the negative binomial (or Pascal) distribution (“Negative Binomial Distribution”, <strong>corrected</strong>).

----

The probability mass function (<strong>pmf</strong>) of the negative binomial distribution is given by <strong>Eq. \@ref(eq:negbinom1)</strong>

\begin{equation}
P(X = x \ | \ r,p) = { x+r-1 \choose r-1} p^r (1-p)^x 
(\#eq:negbinom1)
\end{equation}

where $x$ is the number of failures, $r$ is the number of successes, and $p$ is the probability of success.

----

Alternatively, the random number of trials we have seen, $Y$, will also have a negative binomial distribution. In this case, if we let $y=x+r$ for <strong>Eq. \@ref(eq:negbinom1)</strong>, then the probability mass function of $Y$ is given by <strong>Eq. \@ref(eq:negbinom2)</strong>.

\begin{equation}
P(Y = y \ | \ r,p) = { y-1 \choose r-1} p^r (1-p)^{y-r} 
(\#eq:negbinom2)
\end{equation}

where $y$ is the number of trials, $y-r$ is the number of failures, $r$ is the number of successes, and $p$ is the specified probability of success.

----

```{definition, name="Negative Binomial Probability Distribution"}
A random variable $X$ is said to have a <em>negative binomial probability distribution</em> if and only if

\begin{equation}
P(X=x \ | \ r,p) = {x+r-1 \choose r-1 } p^r (1-p)^x
\end{equation}

where $x$ is the number of failures, $r$ is the given number of successes, and $p$ is the given probability of success.
```

----

```{definition, name="Negative Binomial Probability Distribution"}
A random variable $Y$ is said to have a <em>negative binomial probability distribution</em> if and only if

\begin{align}
\ \ \ \ P(Y=y \ | \ r,p) = {y-1 \choose r-1 } p^r (1-p)^{y-r}
\end{align}

for

\begin{align}
y = r, r+1, r+2, ..., \text{ and } 0 \le p \le 1
\end{align}

where $y$ is the number of trials, $r$ is the given number of successes, and $p$ is the given probability of success.
```

----

```{theorem, name="Negative Binomial Probability Distribution"}
If $X$ is a random variable with a negative binomial distribution,

\begin{equation}
\mu = E(X) = \frac {rq}{p} \ \ \text{ and } \ \ \sigma^2 = V(X) = \frac {rq}{p^2}.
\end{equation}

where $x$ is the number of failures, $r$ is the given number of successes,$p$ is the given probability of success, and $q=(1-p)$ is the probability of failure.  
```


----

```{theorem, name="Negative Binomial Probability Distribution"}
If $Y$ is a random variable with a negative binomial distribution,

\begin{equation}
\mu = E(Y) = \frac rp \ \ \text{ and } \ \ \sigma^2 = V(Y) = \frac {rq}{p^2}.
\end{equation}

where $y$ is the number of trials, $r$ is the given number of successes, $p$ is the given probability of success, and $q=(1-p)$ is the probability of failure.  
```

----

```{r, echo=FALSE, message=FALSE}
library(reticulate)
py_install("scipy")
py_install("matplotlib")

source_python("combo.py")
```

```{python, echo=FALSE, message=FALSE, warning=FALSE}
# Grab our required libraries
from scipy.stats import nbinom
import matplotlib.pyplot as plt

# Create our handles to the plot
_, ax = plt.subplots(1,1)

# Set up our problem parameters, including
# bar colors (color), bar width (width), x-shift (spacing),
# probability (p), successes (r)
#
color='blue'
width=0.5
spacing=0.0
p=1/5.
r=5

# Set up X and Y vectors, and then plot
#
X = list(range(0,60))
Y = [combo(k+r-1,r-1)*p**r*(1-p)**k for k in X ]
X = [x+spacing for x in X]

_ = ax.bar(X, Y, color=color, width=width, label='custom')

# Set up our problem parameters, including
# bar colors (color), bar width (width), x-shift (spacing),
# probability (p), successes (r)
#
color='red'
width=0.5
spacing=0.5
p=1/5.
r=5

# Set up X and Y vectors, and then plot
#
X = list(range(0,60))
Y = nbinom.pmf(X, r, p)#[combo(k+r-1,r-1)*p**r*(1-p)**k for k in X ]
X = [x+spacing for x in X]

_ = ax.bar(X, Y, color=color, width=width, label='binom')

# Tee up our legend
_ = ax.legend(loc='best', frameon=False)

plt.show()

```

---

Computing the average value

```{python, echo=FALSE, message=FALSE, warning=FALSE}
# Grab our required libraries
from scipy.stats import nbinom

# Set up our problem parameters, including
# probability (p), successes (r)
#
p=1/5.
r=5

# Set up by equation for mu and var
print ("mu={:.2f}, var={:.2f} by equation OUCH!".format(p*r/(1-p),p*r/(1-p)**2))

# Set up X and Y vectors, and then plot
#
X = list(range(0,500))
Y = [k * combo(k+r-1,r-1)*p**r*(1-p)**k for k in X ]
mu = sum(Y)
Y = [(k-mu)**2 * combo(k+r-1,r-1)*p**r*(1-p)**k for k in X ]
var = sum(Y)
print ("mu={:.2f}, var={:.2f} by custom".format(mu,var))

# Set up our problem parameters, including
# probability (p), successes (r)
#
p=1/5.
r=5

# Set up X and Y vectors, and then plot
#
X = list(range(0,500))
Y = nbinom.pmf(X, r, p)#[combo(k+r-1,r-1)*p**r*(1-p)**k for k in X ]
mu, var = nbinom.stats(r, p, moments='mv')
print ("mu={:.2f}, var={:.2f} by nbinom".format(mu,var))
```

----

# Personal Notes

The negative probability distribution is equivalent to the geometric distribution when the number of successes equals one. That is, a random variable with a negative binomial probability distribution originates from a context similar to that of a random variable with a geometric distribution:

- The random variable is based on a series of independent Bernoulli TRIALS. 
- Each Bernoulli trial has one of two possible outcomes: SUCCESS or FAILURE. 

The geometric and negative binomial distributions differ as to their desired final outcome:

- Geometric distribution: The random variable equals the total number of trials up to and including the first SUCCESS;
- Negative Binomial (<strong>NOT EXACTLY</strong>): The random variable equals the total number of trials up to and including the $n^{th}$ SUCCESS.

<strong>Not Exactly:</strong> Clearly, a negative binomial distribution equals the geometric distribution when the $Nth$ trial IS the $1st$ TRIAL.

----

Another way to look at the <strong>pmf</strong> is to arrange the sequence of trials into two Events $A$ and $B$, where
the Event $A$ contains all the trials up to but not including the last trial and Event $B$ contains only the last trial. See <strong>Figure 1</strong> below.

![<strong>Figure 1.</strong> Separate the trials into two Events A and B.](/negative-binomial-distribution/example-2.png)

Event $A$ contains the number of trials equal to the total number of failures $k$ plus the total number of successes $r$ minus one,

\begin{align}
N[A] = & k+r-1 \\\\[0.5em]
     = & k + (r-1) \\\\
\end{align}

The probability of Event $A$ is the total probability of any combination of $k$ failures and $r-1$ successes,

\begin{equation}
Pr[A] = {k+r-1 \choose r-1 } (1-p)^k p^{r-1}
\end{equation}

The combination denotes the total number of ways to choose $k+r-1$ trials taken $r-1$ at a time. Essentially, I have $k+r-1$ trials, and then I am allocating $r-1$ successes among those trials.

And for any one of those combinations, I have the combined probability of the $k$ independent failures, $(1-p)^k$, times the combined probability of the $r-1$ independent successes.

The probability of Event $B$ is simply

\begin{equation}
Pr[B] = p
\end{equation}

And because we assume these trials are independent, the total probability is simply

\begin{align}
Pr[X=k] = & Pr[A] \cdot Pr[B] \\\\[1em]
        = & \left[ {k+r-1 \choose r-1 } (1-p)^k p^{r-1} \right] \cdot Pr[B] \\\\[1em]
        = & \left[ {k+r-1 \choose r-1 } (1-p)^k p^{r-1} \right] \cdot  p \\\\[1em]
        = & {k+r-1 \choose r-1 } (1-p)^k p^{r-1} \cdot p \\\\[1em]
        = & {k+r-1 \choose r-1 } (1-p)^k p^r
\end{align}

----

From (Wackerly et al., p.116), let us select fixed values for $y$ and $r$ and consider events $A$ and $B$, where

$
\ \ \ \ \ A = \\{ \text{ the first } (y-1) \text{ trials contain } (r-1) \text{ successes } \\}
$

and

$
\ \ \ \ \ B = \\{ \text{ trial } y \text{ results in a success } \\}
$

Precisely because the events $A$ and $B$ are independent, we can say

$$P[ A \cap B] = P[A] \times P[B]$$

Given a random variable $Y$ with a negative binomial probability distribution and the desired number of $r$ TRIALS that yield SUCCESS, the probability $P$ that $Y$ will equal $y$ trials is given by the expression:

$$P[Y=y] = {y-1 \choose r-1}p^rq^{y-r}$$

where $p$ is the probability of SUCCESS in a trial, $q=1-p$ is the probability of FAILURE, $r$ is the total number of trials, and $y$ is the total number of trials that yield a SUCCESS.

----

### In-Text Citations

(Wackerly et al.)

(“Negative Binomial Distribution”)

### References

Wackerly, Dennis D, et al. Mathematical Statistics with Applications. 2002. 6th ed., Belmont, Calif., Brooks-Cole, 2008.

“Negative Binomial Distribution.” Wikipedia, 18 Dec. 2020, en.wikipedia.org/wiki/Negative_binomial_distribution. Accessed 22 Dec. 2020.
